{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each\n",
    "    # row of a matrix to have unit length\n",
    "\n",
    "    # ## YOUR CODE HERE\n",
    "    all_norm2 = np.sqrt(np.sum(np.power(x, 2), 1))\n",
    "    all_norm2 = 1/all_norm2\n",
    "    x = x * all_norm2[:, np.newaxis]\n",
    "    # ## END YOUR CODE\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "dataset = type('dummy', (), {})()\n",
    "\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0, 4)],\n",
    "    [tokens[random.randint(0, 4)]\n",
    "     for i in xrange(2*C)]\n",
    "\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "v_hat = normalizeRows(np.random.randn(1, 3))[0]\n",
    "dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MYnegSamplingCostAndGradient(predicted,\n",
    "                               target,\n",
    "                               outputVectors,\n",
    "                               random_sample,\n",
    "                               K=10):\n",
    "    sample_vectors = outputVectors[random_sample]\n",
    "    target_pred = outputVectors[target].dot(predicted)\n",
    "    sample_pred = sample_vectors.dot(predicted)\n",
    "    cost = - (np.log(sigmoid(target_pred)) +\n",
    "              np.sum(np.log(sigmoid(-sample_pred))))\n",
    "\n",
    "    gradPred = - sigmoid(- target_pred)*outputVectors[target] + np.dot(\n",
    "        sigmoid(sample_pred), sample_vectors)\n",
    "\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    grad[target] = - sigmoid(- target_pred) * predicted\n",
    "    for i in random_sample:\n",
    "        grad[i] += sigmoid(outputVectors[i].dot(predicted)) * predicted\n",
    "\n",
    "    \n",
    "    # ## END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def ALTnegSamplingCostAndGradient(predicted, target, outputVectors,sampleIndexs,K=10):\n",
    "    V, D = outputVectors.shape\n",
    "    sampleVectors = outputVectors[sampleIndexs, :]\n",
    "    \n",
    "    w_r_out = sigmoid(outputVectors[target].dot(predicted))\n",
    "    w_r_k = sigmoid(- sampleVectors.dot(predicted))\n",
    "    \n",
    "    cost = - np.log(w_r_out) - np.sum(np.log(w_r_k))\n",
    "    gradPred = outputVectors[target] * (w_r_out - 1)+  (1 - w_r_k).dot(sampleVectors)\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    grad[target] = predicted * (w_r_out - 1)\n",
    "    for i in xrange(K):\n",
    "#     grad[sampleIndexs, :] = predicted * ((1 - w_r_k).reshape(K, 1))\n",
    "        grad[sampleIndexs[i]] += predicted * (1 - w_r_k)[i]     \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 3, 0, 4]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 1 \n",
    "K = 5 \n",
    "random_sample = []\n",
    "while len(random_sample) < K:\n",
    "    pick_idx = dataset.sampleTokenIdx()\n",
    "    if pick_idx != target:\n",
    "        random_sample.append(pick_idx)\n",
    "random_sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.7319514233077671,\n",
       " array([-2.13918848,  0.37526311,  0.14503597]),\n",
       " array([[-0.38785727,  0.19586566, -0.9666187 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [-0.25765128,  0.13011239, -0.64211906],\n",
       "        [-0.40008065,  0.2020384 , -0.99708185],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.12623282, -0.06374684,  0.3145977 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYnegSamplingCostAndGradient(v_hat,6,dummy_vectors,random_sample,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.7319514233077671,\n",
       " array([-2.13918848,  0.37526311,  0.14503597]),\n",
       " array([[-0.38785727,  0.19586566, -0.9666187 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [-0.25765128,  0.13011239, -0.64211906],\n",
       "        [-0.40008065,  0.2020384 , -0.99708185],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.12623282, -0.06374684,  0.3145977 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALTnegSamplingCostAndGradient(v_hat,6,dummy_vectors,random_sample,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8284628510959546"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "0.529893462778*-0.96735714  + 0.529893462778*-0.59609459+ 0.529893462778*-0.56713774\n",
    "+ 0.529893462778*-0.96735714  + 0.529893462778*-0.59609459 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
