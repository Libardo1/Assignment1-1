{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each\n",
    "    # row of a matrix to have unit length\n",
    "\n",
    "    # ## YOUR CODE HERE\n",
    "    all_norm2 = np.sqrt(np.sum(np.power(x, 2), 1))\n",
    "    all_norm2 = 1/all_norm2\n",
    "    x = x * all_norm2[:, np.newaxis]\n",
    "    # ## END YOUR CODE\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print(\"Testing normalizeRows...\")\n",
    "    x = normalizeRows(np.array([[3.0, 4.0], [1, 2]]))\n",
    "    # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "    assert(x.all() == np.array([[0.6, 0.8], [0.4472, 0.8944]]).all())\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "def softmaxCostAndGradient1(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "\n",
    "    # Implement the cost and gradients for one predicted word vector\n",
    "    # and one target word vector as a building block for word2vec\n",
    "    # models, assuming the softmax prediction function and cross\n",
    "    # entropy loss.\n",
    "\n",
    "    # Inputs:\n",
    "    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in\n",
    "    #   the written component or \\hat{r} in an earlier version)\n",
    "    # - target: integer, the index of the target word\n",
    "    # - outputVectors: \"output\" vectors (as rows) for all tokens\n",
    "    # - dataset: needed for negative sampling, unused here.\n",
    "\n",
    "    # Outputs:\n",
    "    # - cost: cross entropy cost for the softmax word prediction\n",
    "    # - gradPred: the gradient with respect to the predicted word\n",
    "    #        vector\n",
    "    # - grad: the gradient with respect to all the other word\n",
    "    #        vectors\n",
    "\n",
    "    # We will not provide starter code for this function, but feel\n",
    "    # free to reference the code you previously wrote for this\n",
    "    # assignment!\n",
    "\n",
    "    # ## YOUR CODE HERE\n",
    "    v_hat = predicted\n",
    "    o = target\n",
    "    U = outputVectors\n",
    "    y_hat = (softmax(U.dot(v_hat))).flatten()\n",
    "    y = np.zeros(U.shape[0])\n",
    "    y[o] = 1\n",
    "    cost = np.sum(y * np.log(y_hat)) * -1\n",
    "\n",
    "    def del_cost_del_v_hat(i):\n",
    "        subtraction = y_hat - y\n",
    "        u_w_i = U.T[i]\n",
    "        result = subtraction * u_w_i\n",
    "        return np.sum(result)\n",
    "\n",
    "    def del_cost_del_U_i(i):\n",
    "        return v_hat*(y_hat[i] - y[i])\n",
    "\n",
    "    def get_grad(array, grad_function):\n",
    "        matrix = np.array(array, copy=True)\n",
    "        for i in range(array.shape[0]):\n",
    "                matrix[i] = grad_function(i)\n",
    "        return matrix\n",
    "\n",
    "    gradPred = get_grad(v_hat, del_cost_del_v_hat)\n",
    "    grad = get_grad(U, del_cost_del_U_i)\n",
    "    # ## END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def softmaxCostAndGradient2(predicted, target, outputVectors, dadta):\n",
    "    V, D = outputVectors.shape\n",
    "    scores = softmax(outputVectors.dot(predicted).reshape(1, V)).reshape(V,)\n",
    "    cost = - np.log(scores[target])\n",
    "    \n",
    "    labels = np.zeros(V)\n",
    "    labels[target] = 1\n",
    "    dscores = scores - labels\n",
    "    gradPred = dscores.dot(outputVectors)\n",
    "    grad = dscores.reshape(V, 1).dot(predicted.reshape(D, 1).T)    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test1(params):\n",
    "    predicted = params[0:2]\n",
    "    U = np.reshape(params[2:6],(2,2))\n",
    "    o =  np.random.randint(0, 2, 1)[0]\n",
    "    cost,gradpre,gradU = softmaxCostAndGradient1(predicted,o,U,None)\n",
    "    grad = np.concatenate((gradpre,gradU.flatten()))\n",
    "    return cost,grad\n",
    "\n",
    "def test2(params):\n",
    "    predicted = params[0:2]\n",
    "    U = np.reshape(params[2:6],(2,2))\n",
    "    o =  np.random.randint(0, 2, 1)[0]\n",
    "    cost,gradpre,gradU = softmaxCostAndGradient2(predicted,o,U,None)\n",
    "    grad = np.concatenate((gradpre,gradU.flatten()))\n",
    "    return cost,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,): \n",
      "            Your gradient = 0.146516821704\n",
      "            Numerical gradient = 480.31028104\n",
      "fx =0.742330552069\n",
      "fxh_plus =0.742345203849\n",
      "fxh_minus =0.646283147641\n",
      "(1,): \n",
      "            Your gradient = 0.0168039088191\n",
      "            Numerical gradient = -480.302801674\n",
      "fx =0.742330552069\n",
      "fxh_plus =0.646268311345\n",
      "fxh_minus =0.742328871679\n",
      "(2,): \n",
      "            Your gradient = 0.153214912672\n",
      "            Numerical gradient = 480.310587782\n",
      "fx =0.742330552069\n",
      "fxh_plus =0.742345873667\n",
      "fxh_minus =0.64628375611\n",
      "(3,): \n",
      "            Your gradient = 0.233700107342\n",
      "            Numerical gradient = 480.314273648\n",
      "fx =0.742330552069\n",
      "fxh_plus =0.742353922328\n",
      "fxh_minus =0.646291067598\n",
      "(4,): \n",
      "            Your gradient = -0.153214912672\n",
      "            Numerical gradient = -480.310587782\n",
      "fx =0.742330552069\n",
      "fxh_plus =0.64628375611\n",
      "fxh_minus =0.742345873667\n",
      "(5,): \n",
      "            Your gradient = -0.233700107342\n",
      "            Numerical gradient = 0.212295248336\n",
      "fx =0.742330552069\n",
      "fxh_plus =0.646291067598\n",
      "fxh_minus =0.646248608548\n"
     ]
    }
   ],
   "source": [
    "test_matrix = (normalizeRows(np.random.random_sample((1,6)))).flatten()\n",
    "_ =gradcheck_naive(test1,test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,): \n",
      "            Your gradient = -0.133097179127\n",
      "            Numerical gradient = 480.31028104\n",
      "fx =0.646269837825\n",
      "fxh_plus =0.742345203849\n",
      "fxh_minus =0.646283147641\n",
      "(1,): \n",
      "            Your gradient = -0.0152648196713\n",
      "            Numerical gradient = 0.0168039088194\n",
      "fx =0.646269837825\n",
      "fxh_plus =0.742332232461\n",
      "fxh_minus =0.742328871679\n",
      "(2,): \n",
      "            Your gradient = -0.139181784314\n",
      "            Numerical gradient = -480.296554654\n",
      "fx =0.646269837825\n",
      "fxh_plus =0.646255919753\n",
      "fxh_minus =0.742315230684\n",
      "(3,): \n",
      "            Your gradient = -0.212295248335\n",
      "            Numerical gradient = 0.23370010734\n",
      "fx =0.646269837825\n",
      "fxh_plus =0.742353922328\n",
      "fxh_minus =0.742307182306\n",
      "(4,): \n",
      "            Your gradient = 0.139181784314\n",
      "            Numerical gradient = 480.296554654\n",
      "fx =0.646269837825\n",
      "fxh_plus =0.742315230684\n",
      "fxh_minus =0.646255919753\n",
      "(5,): \n",
      "            Your gradient = 0.212295248335\n",
      "            Numerical gradient = -0.233700107341\n",
      "fx =0.646269837825\n",
      "fxh_plus =0.742307182306\n",
      "fxh_minus =0.742353922328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.00027711,  0.03206873,  0.99971022,  0.44599536,  0.99971022,\n",
       "        0.44599536])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradcheck_naive(test2,test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-108001297.0725"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
